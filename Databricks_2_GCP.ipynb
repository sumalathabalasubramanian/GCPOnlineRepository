{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612f03f1-5838-4a4d-aa07-1759d8e49078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usecase #1 - Data Transfer between HDFS/DBFS to GCS and Vice versa\n(a) Read from HDFS/DBFS and write to GCS.\nHDFS/DBFS Read Completed Successfully\nhdfscnt:  13\n20240717172073\nGCS Write Completed Successfully\n(b) Read from GCS and display count in Databricks.\n+-------+-------+---------+---+--------------------+\n| custid|  fname|    lname|age|          profession|\n+-------+-------+---------+---+--------------------+\n|4009987|   Todd|      Fox| 29|          Politician|\n|4009988|Kathryn|McPherson| 28|Human resources a...|\n+-------+-------+---------+---+--------------------+\nonly showing top 2 rows\n\nReconciliation between DBFS and GCS count....\nGCS Write Completed Successfully including Data Quality/Reconcilation check completed (equivalent to sqoop --validate)\nUsecase #2 - Data Transfer between Hive/Databricks database tables to GCS and Vice versa\n(a) Writing data from GCS to Hive table\nGCS to Hive Write Completed Successfully\n(b) Reading data from hive table\nHive to GCS Write Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# define spark configuration object\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"HDFS -> GCS & Hive -> GCS Read/Write Usecase 1 & 2\") \\\n",
    "    .config(\"spark.jars\", \"dbfs:/FileStore/jar/gcs_connector_hadoop2_2_2_7.jar\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "#conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "#conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "#conf.set(\"google.cloud.auth.service.account.json.keyfile\",\"/home/hduser/weekday-32-proj-427802-a6-fd822f730117.json\")\n",
    "\n",
    "\n",
    "#Need small fixes to make below work\n",
    "#Databricks configs\n",
    "#spark.sparkContext._conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key.id\",\"xxx\")\n",
    "#spark.sparkContext._conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key\",\"-----BEGIN PRIVATE KEY-----\\nxxx\\n-----END PRIVATE KEY-----\\n\")\n",
    "#spark.sparkContext._conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\",\"true\")\n",
    "#spark.sparkContext._conf.set(\"spark.hadoop.fs.gs.project.id\",\"xxx\")\n",
    "#spark.sparkContext._conf.set(\"spark.hadoop.fs.gs.auth.service.account.email\",\"xxx\")\n",
    "\n",
    "conf = spark.sparkContext.getConf().getAll()\n",
    "#print(conf)\n",
    "\n",
    "print(\"Usecase #1 - Data Transfer between HDFS/DBFS to GCS and Vice versa\")\n",
    "\n",
    "print(\"(a) Read from HDFS/DBFS and write to GCS.\")\n",
    "#HDFS\n",
    "#hdfs_df=spark.read.csv(\"hdfs://localhost:54310/user/hduser/datatotransfer/\")\n",
    "#DBFS\n",
    "hdfs_df = spark.read.csv(\"dbfs:/FileStore/datatotransfer/\")\n",
    "print(\"HDFS/DBFS Read Completed Successfully\")\n",
    "\n",
    "hdfscnt=hdfs_df.count()\n",
    "print(\"hdfscnt: \",hdfscnt)\n",
    "curts = spark.createDataFrame([1], IntegerType()).withColumn(\"curts\", current_timestamp()).select(date_format(col(\"curts\"), \"yyyyMMddHHmmSS\")).first()[0]\n",
    "print(curts)\n",
    "\n",
    "#Irfan's bucket\n",
    "#hdfs_df.coalesce(1).write.csv(\"gs://inceptez-data-store/sumalatha_custdata_\"+curts)\n",
    "#Suma's bucket\n",
    "hdfs_df.coalesce(1).write.csv(\"gs://wd32-sumalatha_10/sumalatha1_custdata_\"+curts)\n",
    "print(\"GCS Write Completed Successfully\")\n",
    "\n",
    "\n",
    "print(\"(b) Read from GCS and display count in Databricks.\")\n",
    "#gcs_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \",\").csv(\"gs://inceptez-data-store/test_custdata_\"+curts).toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "gcs_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \",\").csv(\"gs://wd32-sumalatha_10/sumalatha1_custdata_\"+curts).toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "gcs_df.cache()\n",
    "gcs_df.show(2)\n",
    "gcscnt=gcs_df.count()\n",
    "#Reconcilation\n",
    "print(\"Reconciliation between DBFS and GCS count....\")\n",
    "if (hdfscnt==gcscnt):\n",
    "    print(\"GCS Write Completed Successfully including Data Quality/Reconcilation check completed (equivalent to sqoop --validate)\")\n",
    "else:\n",
    "    print(\"Count is not matching - Possibly GCS Write Issue\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Usecase #2 - Data Transfer between Hive/Databricks database tables to GCS and Vice versa\")\n",
    "print(\"(a) Writing data from GCS to Hive table\")\n",
    "gcs_df.write.mode(\"overwrite\").saveAsTable(\"default.custs\")\n",
    "print(\"GCS to Hive Write Completed Successfully\")\n",
    "\n",
    "\n",
    "print(\"(b) Reading data from hive table\")\n",
    "df_hive=spark.read.table(\"default.studentrecords\")\n",
    "df_hive.write.json(\"gs://wd32-sumalatha_10/students_json_\"+curts)\n",
    "print(\"Hive to GCS Write Completed Successfully\")\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1414924076289512,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GCPOnlineTransfer - Usecase 1 & 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
